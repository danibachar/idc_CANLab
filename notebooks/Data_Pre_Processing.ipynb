{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDC CANLab tool\n",
    "\n",
    "This notebook is designed for the eye tracking lab - School of Psychology, the Interdisciplinary Center (IDC) Herzliya. The goal of this tool, is to ease the data cleaning process while for a psychology research.\n",
    "\n",
    "\n",
    "This notebook is build from the following sections:\n",
    "1.   Prerequisite\n",
    "2.   List item\n",
    "\n",
    "\n",
    "\n",
    "If this is your first time using google colab, we highly recommend going over this short tutorial ([link](https://colab.research.google.com/notebooks/intro.ipynb#scrollTo=5fCEDCU_qrC0)) and video ([link](https://www.youtube.com/watch?v=inN8seMm7UI))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Prerequisite\n",
    "This section is **mandatory**, and should be excuted before each use of the notebook. This section includes importing used packages, environment setup and connecting to the IDC_CANLab repository. \n",
    "This section has 4 subsection: 1.1 - 1.4.\n",
    "\n",
    "**Note:** This is a **mandatory** section, which you should run in the beginning of every session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 1.1 (GitHub)\n",
    "!git clone https://github.com/danibachar/idc_CANLab.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a crucial step, please paste here the passphere you have recieved and run the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo `Please Replace all of this with the passphere` | gpg --batch --yes --passphrase-fd 0 idc_CANLab/notebooks/data.json.gpg\n",
    "!cp idc_CANLab/notebooks/data.json ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 1.2 (IDC_CANLab repository)\n",
    "!pip3 install -r idc_CANLab/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development only, for auto reload changes in .py files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# subsection 1.3 (imports)\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "from idc_CANLab.functions import utils,plots,consts, summary, backup # Our functionalities\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual#, FileUpload\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import FileLink, FileLinks\n",
    "# Data manipulation and calculation packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 1.4 (data sturctures)\n",
    "dfs = {}\n",
    "data_frame_names = []\n",
    "cleaning_dfs = {}\n",
    "cleaning_file_names = []\n",
    "print(\"Done preparing DS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you wish to import directly from you Google Drive, please run the following and follow the instructions to authenticate you Google Drive Account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Research Environment\n",
    "This section is **mandatory**, and should be excuted after section 1 and before all other sections of the notebook. This section includes building the research environment.\n",
    "\n",
    "Special instructions:\n",
    "- run subsection 2.1.\n",
    "- fill both text box after subsection 2.1. is done running\n",
    "- run subsection 2.2.\n",
    "\n",
    "![](attachment:https://github.com/danibachar/idc_CANLab/blob/master/images/env_setup.png?raw=true)\n",
    "\n",
    "This section has 2 subsection: 2.1. - 2.2.\n",
    "\n",
    "**Note:** This is a **mandatory** section.\n",
    "\n",
    "**Note:** Make sure to run subsection 2.2. after you fill the information in subsection 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 2.1. (research information)\n",
    "researcher_name = widgets.Text(\n",
    "    value=utils.get_random_string(10),\n",
    "    placeholder='Type your name'\n",
    ")\n",
    "project_name = widgets.Text(\n",
    "    value=utils.get_random_string(6),\n",
    "    placeholder='Type your project name'\n",
    ")\n",
    "\n",
    "names = widgets.VBox([researcher_name,project_name])\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 2.2. (research environment)\n",
    "os.environ[consts.PROJECT_NAME_ENV_VAR]=project_name.value\n",
    "os.environ[consts.USERNAME_ENV_VAR]=researcher_name.value\n",
    "print(\"Done loading environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Primary Files\n",
    "In this section we will open the primary research file(s). **Please note**, the system is a session based system, that is, it saves the data for a undefined time after you finish the session. Thus, we recommend, if you upolad your data in the past, **go over subsection 3.1.1 - 3.1.2.** and check if your data is saved, if so, all you need to do, is to load the data to the notebook. **Otherwise, jump to subsections 3.2.1.-3.2.2.** and upload the data to the system.\n",
    "\n",
    "\n",
    "**Comments:**\n",
    "1. You can choose here more than one file, but the files must have the same structure (What we will do is taking all the files and concat them together.)\n",
    "2. Please note we currently support only CSV files\n",
    "3. If your'e using Excel or other file formats please convert to CSV\n",
    "    - Excel to CSV ([link](https://knowledgebase.constantcontact.com/articles/KnowledgeBase/6409-saving-an-excel-file-as-a-csv-file?lang=en_US))\n",
    "    - EDF to CSV ([link](https://emotiv.gitbook.io/emotivpro/convert_edf_to_csv))\n",
    "4. For the primary data frame, in subsection 3.1. and 3.3. , you can choose more than one file, this is done in case when you want to concat a number of files. **Please note**  in this case, files must have the same structure or else it will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 3.1 - Uploaded Primary Files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subsection 3.1. (pick primary past uploaded file)\n",
    "def choose_filename(filename):\n",
    "  return filename\n",
    "primary_file_name_dropdown = widgets.SelectMultiple(\n",
    "    options=utils.list_all_files_in_dir(\".\"),\n",
    "    description='File name:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "# primary_file_name_dropdown\n",
    "interact(choose_filename, filename=primary_file_name_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 3.2. (load primary past uploaded file)\n",
    "primary_file_names = list(primary_file_name_dropdown.value)\n",
    "un_concat_dfs = []\n",
    "dfs_columes = []\n",
    "for file_name in primary_file_names:\n",
    "    df = pd.read_csv(file_name)\n",
    "    dfs_columes.append(df.columns)\n",
    "    un_concat_dfs.append(df)\n",
    "\n",
    "# Validate same columns\n",
    "for col1 in dfs_columes:\n",
    "    for col2 in dfs_columes:\n",
    "        assert set(col1) == set(col2), \"The columns in the data files must be identical!\"\n",
    "\n",
    "dfs['primary'] = pd.concat(un_concat_dfs)\n",
    "data_frame_names = list(dfs.keys())\n",
    "\n",
    "print(summary.info_dfs(list(dfs.values())))\n",
    "for sum_url in summary.describe_dfs(list(dfs.values())):\n",
    "    print(sum_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 3.2 - Upload New Primary Files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 3.2.1. (pick primary file to upload)\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 3.2.2. (upload primary file)\n",
    "un_concat_dfs = []\n",
    "dfs_columes = []\n",
    "for file_name in primary_file_names:\n",
    "    df = pd.read_csv(file_name)\n",
    "    dfs_columes.append(df.columns)\n",
    "    un_concat_dfs.append(df)\n",
    "\n",
    "# Validate same columns\n",
    "for col1 in dfs_columes:\n",
    "    for col2 in dfs_columes:\n",
    "        assert set(col1) == set(col2)\n",
    "\n",
    "dfs['primary'] = pd.concat(un_concat_dfs)\n",
    "data_frame_names = list(dfs.keys())\n",
    "\n",
    "print(summary.info_dfs(list(dfs.values())))\n",
    "for sum_url in summary.describe_dfs(list(dfs.values())):\n",
    "    print(sum_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Other Files (to be merged with primary)\n",
    "\n",
    "In this section we will upload files to be merged with the primary research files (from section 3) and merge them. In the end of this section, you will have one merged file. [For example, one can marge betwen raw data and memory span, age, etc].\n",
    "\n",
    "An INNER JOIN (**merge**) is such type of join that returns all rows from both the participating tables (flies) where the key record of one table (primary) is equal to the key records of another table (other file). This type of join required a comparison operator to match rows from the participating tables based on a common field or column of both the tables. This operation is **not mandatory** and is done in subsections 4.5.-4.6.\n",
    "\n",
    "![image](https://i.stack.imgur.com/cQZCg.png)\n",
    "\n",
    "**Please note**, the system is a session based system, that is, it saves the data for a undefined time after you finish the session. Thus, we recommend, if you upolad your data in the past, **go over subsection 4.1.1. - 4.1.2.** and check if your data is saved, if so, all you need to do, is to load the data to the notebook. **Otherwise, jump to subsections 4.2.1.-4.2.2.** and upload the data to the system. **Following, go over to section 4.3.1-4.3.2 to merge the choosen files with the primary**\n",
    "\n",
    "\n",
    "Comments:\n",
    "1. Please note we currently support only CSV files\n",
    "2. If your'e using Excel or other file formats please convert to CSV\n",
    "    - Excel to CSV ([link](https://knowledgebase.constantcontact.com/articles/KnowledgeBase/6409-saving-an-excel-file-as-a-csv-file?lang=en_US))\n",
    "    - EDF to CSV ([link](https://emotiv.gitbook.io/emotivpro/convert_edf_to_csv))\n",
    "3. Please note, we highly recommend going over this short tutorial ([link](https://riptutorial.com/sql/example/22934/join-terminology--inner--outer--semi--anti---)), about JOIN(merge) and ANTI_JOIN(drop rows) opparations in SQL and Python Pandas to fully understand.\n",
    "4. You can choose here more than one file, but the every file must comply with INNER JOIN specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 4.1 - Uploaded Other Files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 4.1.1. (pick other past uploaded files to be merged) \n",
    "def choose_filename(filename):\n",
    "  return filename\n",
    "secondary_files_name_dropdown = widgets.SelectMultiple(\n",
    "    options=utils.list_all_files_in_dir(\".\"),\n",
    "    description='File name:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "interact(choose_filename, filename=secondary_files_name_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 4.1.2. (load other past uploaded files to be merged)\n",
    "scondary_file_names = list(secondary_files_name_dropdown.value)\n",
    "for file_name in scondary_file_names:\n",
    "    data_frame_names.append(file_name)\n",
    "    dfs[file_name] = pd.read_csv(file_name)\n",
    "\n",
    "print(summary.info_dfs(list(dfs.values())))\n",
    "for sum_url in summary.describe_dfs(list(dfs.values())):\n",
    "    print(sum_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 4.2 - Uploaded New Other Files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 4.2.1. (pick other files to upload, that will be merged)\n",
    "# pick and upload **not** primary files (one or more, if more, same column name for all, that is concatenation one above the other)\n",
    "merged_uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 4.2.2. (upload other files, that will be merged)\n",
    "for file_name, byte_file in merged_uploaded.items():\n",
    "  data_frame_names.append(file_name)\n",
    "  dfs[file_name] = pd.read_csv(io.StringIO(byte_file.decode(\"utf-8\")))\n",
    "    \n",
    "print(summary.info_dfs(list(dfs.values())))\n",
    "for sum_url in summary.describe_dfs(list(dfs.values())):\n",
    "    print(sum_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 4.3 - Merge The Files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 4.3.1. (pick columns to merge by)\n",
    "\n",
    "def choose_colums(colums):\n",
    "  return colums\n",
    "\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "external_merge_columns_dropdown = []\n",
    "for i in range(1,len(set(data_frame_names))):\n",
    "  if data_frame_names[i] == \"primary\":\n",
    "    continue\n",
    "  w1 = widgets.SelectMultiple(\n",
    "      options=dfs[data_frame_names[i]].columns,\n",
    "      description=data_frame_names[i]\n",
    "  )\n",
    "  w2 = widgets.SelectMultiple(\n",
    "      options=dfs[data_frame_names[0]].columns,\n",
    "      description=data_frame_names[0]\n",
    "  )\n",
    "  external_merge_columns_dropdown.append(widgets.HBox([w1,w2]))\n",
    "  \n",
    "columns_map = widgets.VBox(external_merge_columns_dropdown)\n",
    "columns_map\n",
    "\n",
    "\n",
    "## For each file we wish to merge, select the columns you wish to merge by with the primary file, \n",
    "## these columns will be used as unique key and the recommendation will be the User \n",
    "## Id/Session Label/Subject and Trial_Id/Trial Label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 4.3.2. (merge the files)\n",
    "merged_df = dfs[data_frame_names[0]].copy()\n",
    "for i in range(1,len(set(data_frame_names))):\n",
    "  child = columns_map.children[i-1]\n",
    "  left = list(child.children[1].value)\n",
    "  right = list(child.children[0].value)\n",
    "  merged_df = merged_df.merge(dfs[data_frame_names[i]], left_on=left, right_on=right, how='inner')\n",
    "\n",
    "print(summary.info_dfs([merged_df]))\n",
    "for sum_url in summary.describe_dfs([merged_df]):\n",
    "    print(sum_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Dropping colums\n",
    "\n",
    "In this section we will remove uneeded columns from the merged file. We highly recommend removing unnecessary column for better performance\n",
    "\n",
    "This section is **not mandatory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 5.1. (choosing column unwanted to be droped)\n",
    "\n",
    "def choose_colums(colums):\n",
    "  return colums\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "colums_dropdown = widgets.SelectMultiple(\n",
    "    options=merged_df.columns,\n",
    "    description='Select colums to drop:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "interact(choose_colums, colums=colums_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 5.2. (Dropping the columns)\n",
    "colums_to_drop = list(colums_dropdown.value)\n",
    "merged_df = merged_df.drop(columns=colums_to_drop)\n",
    "\n",
    "# Flattening \n",
    "for col in merged_df.columns:\n",
    "  if merged_df[col].dtype == object:\n",
    "    merged_df[col] = merged_df[col].astype('category')\n",
    "\n",
    "print(summary.info_dfs([merged_df]))\n",
    "for sum_url in summary.describe_dfs([merged_df]):\n",
    "    print(sum_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 - Converting column (data type)\n",
    "\n",
    "In this section we will convert columns dtype to categorical. [for example subject_id, trial_id, memory span and others should be categorical, plesae see the github documentation on categorical data] \n",
    "\n",
    "This section is **not mandatory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 6.1. (pick columns to convert to categorial (data type))\n",
    "def choose_colums(colums):\n",
    "  return colums\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "categorical_colums_dropdown = widgets.SelectMultiple(\n",
    "    options=merged_df.columns,\n",
    "    description='Select colums to drop:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "interact(choose_colums, colums=categorical_colums_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 6.2. (convert)\n",
    "colums_to_convert_to_categorical = list(categorical_colums_dropdown.value)\n",
    "for col in colums_to_convert_to_categorical:\n",
    "  merged_df[col] = merged_df[col].astype('category')\n",
    "\n",
    "print(summary.info_dfs([merged_df]))\n",
    "for sum_url in summary.describe_dfs([merged_df]):\n",
    "    print(sum_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 - Ploting\n",
    "\n",
    "In this section we will plot the trials a certain Subject was going through. This section is **not mandatory**, but if you choose to use the functionality, we highly recommend going over the below instrections.\n",
    "\n",
    "1. In subsection 7.1. - After running the code, select the name of the column you wish to group by [usually the subject or trial columns]\n",
    "2. In subsection 7.2. - After running the code, select the desire columns you wish to have as your y and x axis. [For y, usually the column that follow the eye gaze on the Target, but could also be for the Competitor or one of the fillers. For x, usually the time column]\n",
    "3. In subsection 7.3. - After running the code,select the different features [e.g. memory span, critical, load, noise level) you want to create the grid by]\n",
    "4. In subsection 7.4. - **After completing subsection 7.1. - 7.3.** run the code for the plot.\n",
    "\n",
    "**Please note** running subsection 7.4. will take a few minuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 7.1. (select column name to group by)\n",
    "def col_name(column_name):\n",
    "  return column_name\n",
    "\n",
    "groupped_column_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Subject:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "interact(col_name, column_name=groupped_column_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 7.2. (select columns for y and x axis)\n",
    "def choose_axis(axis):\n",
    "  return axis\n",
    "\n",
    "y_column_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Select `Y` axis column name:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "x_column_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Select `X` axis column name:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "interact(choose_axis, axis=y_column_dropdown)\n",
    "interact(choose_axis, axis=x_column_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 7.3. (select different features)\n",
    "def choose_colums(colums):\n",
    "  return colums\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "features_column_dropdown = widgets.SelectMultiple(\n",
    "    options=merged_df.columns,\n",
    "    description='Select colums to drop:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "interact(choose_colums, colums=features_column_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 7.4. (plottig)\n",
    "groupping_col_name = groupped_column_dropdown.value\n",
    "y_name = y_column_dropdown.value\n",
    "x_name = x_column_dropdown.value\n",
    "grid_features = list(features_column_dropdown.value)\n",
    "\n",
    "_, url = plots.plots_by_group_and_features(merged_df, groupping_col_name, y_name, x_name, grid_features)\n",
    "print(\"plots_by_group_and_features - \",url)\n",
    "_, url = plots.plot_general_avg_grid(merged_df, y_name, x_name, grid_features)\n",
    "print(\"plot_general_avg_grid - \",url)\n",
    "_, url = plots.plot_general_avg(merged_df, y_name, x_name)\n",
    "print(\"plot_general_avg - \",url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8 - Data Cleaning\n",
    "\n",
    "In this section we will clean different values from different columns. You can repeat the following steps in order to clean different values from different columns. This section is **not mandatory**, but if you choose to use the functionality, we highly recommend going over instrections below and in any subsection that you wish to use.\n",
    "\n",
    "This section has 3 subsection, each has its own instrections, the subsection are:\n",
    "\n",
    "Subsection 8.1. - Drop rows with Nan/Null values\n",
    "\n",
    "Subsection 8.2. - Drop rows with a specific value (in a column)\n",
    "\n",
    "Subsection 8.3. - Drop rows according to exteral files\n",
    "\n",
    "After each row removing the system will provide details on the amount of rows removed and a data summarization and plotting subsection.\n",
    "\n",
    "\n",
    ") -  **Running this subsection will remove `rows` that contains null/nan values.** The system will provide details on the amount of rows removed\n",
    "2. In subsection 8.2.1-8.2.3 (Drop rows with a specific value in a column)\n",
    "3. In subsection 8.3. -(Drop rows according to exteral files) After running the code,select the different features [e.g. memory span, critical, load, noise level) you want to create the grid by]\n",
    "4. In subsection 8.4. - **After completing subsection 8.1. - 8.3.** run the code for the plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Subsection 8.1 - Drop rows with Nan/Null values\n",
    "\n",
    "In this subsection we will remove all rows that contains null/nan values. **Please note - running the below code will drop the `entire row` if it contains a cell with a null or nan value.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.1.1. (Drop rows with Nan/Null values)\n",
    "before = len(merged_df.index)\n",
    "\n",
    "# Droping\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "after = len(merged_df.index)\n",
    "print(\"Amount of rows BEFORE removing Null/NaN values = {}\".format(before))\n",
    "print(\"Amount of rows AFTER removing Null/NaN values = {}\".format(after))\n",
    "print(\"The total amount of rows removed = {}\".format(abs(before-after)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Subsection 8.2 - Drop rows with a specific value (in a column)\n",
    "\n",
    "In this subsection we will remove all rows that contains a specific values. **Please note - running the below code will drop the `entire row` if it contains the choosen value.** This section is combined from the following subsection, please go over the instructions, before running this section. \n",
    "\n",
    "\n",
    "1. In subsection 8.2.1 (Choose Column to filter by) - after running this code box a dropdwon list will appear with all the column name in the file. choose the column you wish to filter by the optional values you wish to remove.\n",
    "2. In subsection 8.2.2. (Choose value to remove from the data) - after running this code box a dropdwon list will appear with all the diffrent values in the choosen column in the previous subsection, mark the values you wish to remove. \n",
    "3. In subsection 8.2.3. (Remove rows with choosen value) - after running this code box, **all** rows with the choosen value (from the previous subsection 8.2.2.) in the choosen column (from the previous subsection 8.2.1.) will be drop.\n",
    "4. subsection 8.2.4. (Data Summarization and plotting) - run this code box to get the summarization.\n",
    "\n",
    "\n",
    "Comments:\n",
    "1. This section can be implemented as many time as needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.2.1. (Choose Column to filter by)\n",
    "def col_name(column_name):\n",
    "  return column_name\n",
    "\n",
    "filter_column_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Col Name:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "interact(col_name, column_name=filter_column_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.2.2. (Choose value to remove from the data)\n",
    "col_name = filter_column_dropdown.value\n",
    "values = list(merged_df[col_name].unique())\n",
    "def value_name(value):\n",
    "  return value\n",
    "\n",
    "value_name_dropdown = widgets.Dropdown(\n",
    "    options=values,\n",
    "    description='Value Name:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "interact(value_name, value=value_name_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.2.3. (Remove rows with choosen value)\n",
    "before = len(merged_df.index)\n",
    "value = value_name_dropdown.value\n",
    "selector = (merged_df[col_name] != value)\n",
    "merged_df = merged_df[selector]\n",
    "\n",
    "after = len(merged_df.index)\n",
    "print(\"Amount of rows BEFORE removing choosen values = {}\".format(before))\n",
    "print(\"Amount of rows AFTER removing choosen values = {}\".format(after))\n",
    "print(\"The total amount of rows removed = {}\".format(abs(before-after)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.2.4. (Data Summarization and plotting)\n",
    "print(summary.info_dfs([merged_df]))\n",
    "for sum_url in summary.describe_dfs([merged_df]):\n",
    "    print(sum_url)\n",
    "\n",
    "_, url = plots.plots_by_group_and_features(merged_df, groupping_col_name, y_name, x_name, grid_features)\n",
    "print(\"plots_by_group_and_features - \",url)\n",
    "_, url = plots.plot_general_avg_grid(merged_df, y_name, x_name, grid_features)\n",
    "print(\"plot_general_avg_grid - \",url)\n",
    "_, url = plots.plot_general_avg(merged_df, y_name, x_name)\n",
    "print(\"plot_general_avg - \",url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Subsection 8.3 - Drop rows according to external files\n",
    "\n",
    "In this subsection we will remove all rows according to an exteral file. First, we will upload the extrnal files (we will use the idea behind **ANTI JOIN** to drop certain rows accroding to files you have prepared in advance)\n",
    "\n",
    "An ANTI JOIN is a form of join with reverse logic. Instead of returning rows when there is a match (according to the join predicate) between the left (primary file) and right side (external file), an anti-join returns those rows from the left side of the predicate for which there is no match on the right. This behavior is exactly that of a NOT IN subquery with the right side of the anti-join predicate corresponding to the subquery. This operation is **not mandatory**.\n",
    "\n",
    "![image](https://i.stack.imgur.com/V0isI.png)\n",
    "\n",
    "**Please note**, the system is a session based system, that is, it saves the data for a undefined time after you finish the session. Thus, we recommend, if you upolad your data in the past, **go over subsection 8.3.1.1. - 8.3.1.2.** and check if your data is saved, if so, all you need to do, is to load the data to the notebook. **Otherwise, jump to subsections 8.3.2.1-8.3.2.2.** and upload the data to the system. **Following, go over to section 8.3.3.1-8.3.3.2 to drop according to external file from the primary file**\n",
    "\n",
    "\n",
    "Comments:\n",
    "1. Please note we currently support only CSV files\n",
    "2. If your'e using Excel or other file formats please convert to CSV\n",
    "    - Excel to CSV ([link](https://knowledgebase.constantcontact.com/articles/KnowledgeBase/6409-saving-an-excel-file-as-a-csv-file?lang=en_US))\n",
    "    - EDF to CSV ([link](https://emotiv.gitbook.io/emotivpro/convert_edf_to_csv))\n",
    "3. Please note, we highly recommend going over this short tutorial ([link](https://riptutorial.com/sql/example/22934/join-terminology--inner--outer--semi--anti---)), about JOIN(merge) and ANTI_JOIN(drop rows) opparations in SQL and Python Pandas to fully understand.\n",
    "4. You can choose here more than one file, but the every file must comply with ANTI JOIN specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 8.3.1 - Uploaded External Files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.3.1.1 (pick external past uploaded files)\n",
    "def choose_filename(filename):\n",
    "  return filename\n",
    "anti_join_files_name_dropdown = widgets.SelectMultiple(\n",
    "    options=utils.list_all_files_in_dir(\".\"),\n",
    "    description='File name:',\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "interact(choose_filename, filename=anti_join_files_name_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.3.1.2. (load external past files)\n",
    "cleaning_dfs = {}\n",
    "anti_join_file_names = list(anti_join_files_name_dropdown.value)\n",
    "for file_name in anti_join_file_names:\n",
    "  c_df = pd.read_csv(file_name)\n",
    "  for col in c_df.columns:\n",
    "    if col in merged_df.columns:\n",
    "      c_df[col] = c_df[col].astype(merged_df[col].dtype)\n",
    "  cleaning_dfs[file_name] = c_df\n",
    "cleaning_file_names = list(cleaning_dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 8.3.2 - Uploaded New External Files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.3.2.1. (pick external files to upload)\n",
    "# pick and upload **not** primary files (one or more, if more, same column name for all, that is concatenation one above the other)\n",
    "anti_merge_uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.3.2.2. (upload external files)\n",
    "for file_name, byte_file in anti_merge_uploaded.items():\n",
    "  c_df = pd.read_csv(io.StringIO(byte_file.decode(\"utf-8\")))\n",
    "  for col in c_df.columns:\n",
    "    if col in merged_df.columns:\n",
    "      c_df[col] = c_df[col].astype(merged_df[col].dtype)\n",
    "        \n",
    "  cleaning_dfs[file_name] = c_df\n",
    "cleaning_file_names = list(cleaning_dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 8.3.3. - anti-join\n",
    "\n",
    "This section is combined from the following subsection, please go over the instructions, before running this section.\n",
    "\n",
    "1. In subsection 8.3.3.1. (Choose the unique column names) - Choose the unique column names you wish to anti-join. This columns sould create a unique like key so we can tell our notebook to remove each row that corresponds from the file to the data we have cleaning and merged so far.\n",
    "2. In subsection 8.3.3.2. (drop rows according to external files) - after running this code box, all rows, that match, will be drop.\n",
    "3. subsection 8.3.3.3. (Preperation for accuracy summary) - run this code boxes to choose values for the accuracy test.\n",
    "4. subsection 8.3.3.4. (Data Summarization and plotting) - run this code box to get the summarization.\n",
    "\n",
    "\n",
    "This section can be implemented as many time as needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.3.3.1. (Choose the unique column names)\n",
    "def choose_colums(colums):\n",
    "  return colums\n",
    "\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "external_clean_columns_dropdown = []\n",
    "for i in range(0,len(cleaning_file_names)):\n",
    "  w1 = widgets.SelectMultiple(\n",
    "      options=cleaning_dfs[cleaning_file_names[i]].columns,\n",
    "      description=cleaning_file_names[i],\n",
    "      layout={'width': 'max-content'}\n",
    "  )\n",
    "  w2 = widgets.SelectMultiple(\n",
    "      options=merged_df.columns,\n",
    "      description='primary data frame',\n",
    "      layout={'width': 'max-content'}\n",
    "  )\n",
    "  external_clean_columns_dropdown.append(widgets.HBox([w1,w2]))\n",
    "  \n",
    "columns_map = widgets.VBox(external_clean_columns_dropdown)\n",
    "columns_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.3.3.2. (drop rows according to external files)\n",
    "for i in range(len(cleaning_file_names)):\n",
    "    before = len(merged_df.index)\n",
    "    \n",
    "    clean_df = cleaning_dfs[cleaning_file_names[i]]\n",
    "    child = columns_map.children[i-1]\n",
    "    left = list(child.children[1].value)\n",
    "    right = list(child.children[0].value)\n",
    "    merged_df = utils.anti_join(merged_df, clean_df, left_on=left, right_on=right)\n",
    "    \n",
    "    after = len(merged_df.index)\n",
    "    print(\"Amount of rows BEFORE removing choosen values = {}\".format(before))\n",
    "    print(\"Amount of rows AFTER removing choosen values = {}\".format(after))\n",
    "    print(\"The total amount of rows removed = {}\".format(abs(before-after)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.3.3.3. accuracy\n",
    "\n",
    "#1) Choose the groupping by value - this column will be the column we group by, usually the subjects\n",
    "\n",
    "group_by_col_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Column name:',\n",
    "    layout={'width': 'max-content'},\n",
    ")\n",
    "group_by_col_dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Choose the column we wish to check its accuracy - usually we would like to check the accuracy in trials\n",
    "count_by_col_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Column name:',\n",
    "    layout={'width': 'max-content'},\n",
    ")\n",
    "count_by_col_dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Choose the column(s) we wish to grid by - usually the feature/effects\n",
    "test_col_dropdown = widgets.SelectMultiple(\n",
    "    options=merged_df.columns,\n",
    "    description='Column name:',\n",
    "    layout={'width': 'max-content'},\n",
    ")\n",
    "test_col_dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsection 8.3.3.3. (Data Summarization and plotting)\n",
    "print(summary.info_dfs([merged_df]))\n",
    "for sum_url in summary.describe_dfs([merged_df]):\n",
    "    print(sum_url)\n",
    "\n",
    "if group_by_col_dropdown and group_by_col_dropdown.value:\n",
    "    if count_by_col_dropdown and count_by_col_dropdown.value:\n",
    "        if test_col_dropdown and test_col_dropdown.value:\n",
    "            print(summary.accuracy_calculator(df_before, merged_df, group_by_col_dropdown.value,count_by_col_dropdown.value, list(test_col_dropdown.value)))\n",
    "    \n",
    "_, url = plots.plots_by_group_and_features(merged_df, groupping_col_name, y_name, x_name, grid_features)\n",
    "print(\"plots_by_group_and_features - \",url)\n",
    "_, url = plots.plot_general_avg_grid(merged_df, y_name, x_name, grid_features)\n",
    "print(\"plot_general_avg_grid - \",url)\n",
    "_, url = plots.plot_general_avg(merged_df, y_name, x_name)\n",
    "print(\"plot_general_avg - \",url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9 - Saving files\n",
    "\n",
    "In this section we will save the files. **At any stage you can choose to save the current state of our manipulated data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1) Full backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck_merged_df = merged_df.copy()\n",
    "original,cleaning,primary = backup.save_locally_and_update(merged_df, dfs, cleaning_dfs)\n",
    "print(\"Original Files - these files are identical to the ones you uploaded\\ni.e. the primary files, and the extra data files\")\n",
    "for url in original:\n",
    "    print(url)\n",
    "print(\"Cleaning Files - these files are identical to the ones you uploaded to clean with\\ni.e the ones we did the anti-join\")\n",
    "for url in cleaning:\n",
    "    print(url)\n",
    "print(\"Primary File - these files are the ones we cleaning and modified along the notebook\\nThese are the most interesting ones\")\n",
    "for url in primary:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank you for using this note book\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Experimental Below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Extra Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1) Split columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1) Choose column you want to split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_name(col):\n",
    "  return col\n",
    "\n",
    "split_col_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Column name:',\n",
    ")\n",
    "\n",
    "interact(col_name, col=split_col_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1) Choose value to split by, and fill value\n",
    "### For example, we wish to split the `RECORDING_SESSION_LABEL` columns, because we have some subjects that appear as follow `42` and also `42b`. In that case we wish to split `RECORDING_SESSION_LABEL` into 2 columns  `RECORDING_SESSION_LABEL` and `RECORDING_SESSION_LABEL-type`.\n",
    "### Under `RECORDING_SESSION_LABEL` we will have `42` and under `RECORDING_SESSION_LABEL-type` we will have `a` or `b`\n",
    "\n",
    "### Hence are the values we require from you to enter\n",
    "\n",
    "#### 1) Delimiter - in the example aboce it will be `b`\n",
    "#### 2) Suffix - in the example aboce it will be `type`\n",
    "#### 3) Fill value - in the example aboce it will be `a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_delimiter = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type the delimiter'\n",
    ")\n",
    "_suffix = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type the type suffix for the new columns'\n",
    ")\n",
    "\n",
    "_fill_val = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type the fill value'\n",
    ")\n",
    "\n",
    "names = widgets.VBox([_delimiter,_suffix,_fill_val])\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = split_col_dropdown.value\n",
    "delimiter = _delimiter.value\n",
    "suffix = _suffix.value\n",
    "fill_val = _fill_val.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, delimiter, columns):\n",
    "    if x:\n",
    "        d_index = x.find(delimiter)\n",
    "        first = x[:d_index].strip()\n",
    "        second = x[d_index:].strip()\n",
    "    else:\n",
    "        first = None\n",
    "        second = None\n",
    "    return pd.Series([first, second], index=columns)\n",
    "\n",
    "# merged_df.info()\n",
    "test_df = merged_df.copy()\n",
    "new_col_name = col_name+\"-\"+suffix\n",
    "# test_df[col_name],test_df[new_col_name] = test_df[col_name].str.split(delimiter,1).str \n",
    "\n",
    "ccc = [col_name, new_col_name]\n",
    "r[ccc] = test_df[col_name].apply(lambda x: split_data(x, delimiter, ccc))\n",
    "\n",
    "# r = pd.DataFrame(test_df[col_name].str.slice(delimiter,1).tolist(), columns = [col_name, new_col_name])\n",
    "\n",
    "# test_df[new_col_name] = test_df[new_col_name].fillna(fill_val)\n",
    "# test_df[new_col_name] = test_df[new_col_name].fillna(fill_val)\n",
    "# test_df.info()\n",
    "# test_df[new_col_name]\n",
    "r.describe()\n",
    "# r[new_col_name].describe(include=['object'])\n",
    "# r[new_col_name].head()\n",
    "# test_df[test_df[new_col_name] != np.nan][new_col_name].describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2) Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1) Choose column that you would like to sum according\n",
    "#### Usually it will be all the columns that checks for target gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_name(col):\n",
    "  return col\n",
    "\n",
    "sum_col_dropdown = widgets.SelectMultiple(\n",
    "    options=merged_df.columns,\n",
    "    description='Column name:',\n",
    "    layout={'width': 'max-content'},\n",
    ")\n",
    "\n",
    "interact(col_name, col=sum_col_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2) Choose column that you would like filter according\n",
    "#### Usually it will be the Bin Start Time column, we would like to normelize according to when the word ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Column name:',\n",
    "    layout={'width': 'max-content'},\n",
    ")\n",
    "\n",
    "interact(col_name, col=filter_col_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3) Choose column that you would like to pad\n",
    "#### Usually it will be the the target gaze column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_name(col):\n",
    "  return col\n",
    "padding_col_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Column name:',\n",
    "    layout={'width': 'max-content'},\n",
    ")\n",
    "\n",
    "interact(col_name, col=padding_col_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4) Choose value in the column you selected above which you wish to normelaize by\n",
    "#### For example, if we choose Bin Start Time, here we will choose the value of the mean word duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_normelize_value_selection = widgets.IntText(\n",
    "    value=0,\n",
    "    description='Normelize Value:',\n",
    "    disabled=False\n",
    ")\n",
    "padding_normelize_value_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.5) Choose value you wish to pad with\n",
    "#### Usually will be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_value_selection = widgets.IntText(\n",
    "    value=0,\n",
    "    description='Padding Value:',\n",
    "    disabled=False\n",
    ")\n",
    "padding_value_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normelize_value = padding_normelize_value_selection.value\n",
    "filter_col_name = filter_col_dropdown.value\n",
    "summed_cols = list(sum_col_dropdown.value)\n",
    "padding_col_name = padding_col_dropdown.value\n",
    "padding_value = padding_value_selection.value\n",
    "print(\"normelize value = \", normelize_value)\n",
    "print(\"filter_col_name = \", filter_col_name)\n",
    "print(\"summed_cols = \", summed_cols)\n",
    "print(\"padding_col_name = \", padding_col_name)\n",
    "print(\"padding_value = \", padding_value)\n",
    "\n",
    "bck = merged_df.copy()\n",
    "\n",
    "# Suming\n",
    "bck[\"summ_looks\"] = bck[summed_cols].sum(axis=1)\n",
    "# Creating selector and removing values not equal to zero\n",
    "selector1 = (bck[\"summ_looks\"] != 0)\n",
    "selector2 = (bck[filter_col_name] >= normelize_value)\n",
    "selector = selector1 & selector2\n",
    "bck.loc[selector, padding_col_name] = padding_value\n",
    "\n",
    "bck.info()\n",
    "\n",
    "_, url = plots.plots_by_group_and_features(bck, groupping_col_name, y_name, x_name, grid_features)\n",
    "print(\"plots_by_group_and_features - \",url)\n",
    "_, url = plots.plot_general_avg_grid(bck, y_name, x_name, grid_features)\n",
    "print(\"plot_general_avg_grid - \",url)\n",
    "_, url = plots.plot_general_avg(bck, y_name, x_name)\n",
    "print(\"plot_general_avg - \",url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization library  \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(merged_df.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(merged_df.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_int = {j:i for i,j in enumerate(pd.unique(merged_df.values.ravel()))} # like you did\n",
    "n = len(value_to_int)     \n",
    "# discrete colormap (n samples from a given cmap)\n",
    "cmap = sns.color_palette(\"Pastel2\", n) \n",
    "ax = sns.heatmap(merged_df.replace(value_to_int), cmap=cmap)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(merged_df)\n",
    "g.map(plt.scatter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_col_name_list = [col for col in merged_df.columns if (merged_df[col].dtype == object)]\n",
    "\n",
    "# for col in obj_col_name_list:\n",
    "#     print('Column name:', col)\n",
    "#     print(merged_df[col].value_counts())\n",
    "#     print('\\n')\n",
    "    \n",
    "\n",
    "# merged_data_num = merged_df.select_dtypes(exclude=['object'])\n",
    "\n",
    "merged_data_num = merged_df.select_dtypes(exclude=['int64'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Numerical column names:\n",
    "merged_data_num_col_name = merged_data_num.columns # class: 'pandas.core.indexes.base.Index'\n",
    "\n",
    "# Numerical columns with missing values\n",
    "numerical_col_with_missing_values = [col for col in merged_data_num.columns if merged_data_num[col].isnull().any()] # Your code here\n",
    "\n",
    "# Number of missing values in each column \n",
    "merged_data_num_missing_val = (merged_data_num.isnull().sum())\n",
    "\n",
    "print('Summary:',\n",
    "      'Numerical columns with missing values are:',\n",
    "      numerical_col_with_missing_values,\n",
    "      'number of missing values by column:',\n",
    "      merged_data_num_missing_val[merged_data_num_missing_val > 0],\n",
    "     sep='\\n',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
