{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/danibachar/idc_CANLab.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r idc_CANLab/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/google/colab/data_table.py:30: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils import traitlets as _traitlets\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "    \n",
    "from idc_CANLab.functions import utils,plots,consts, summary, backup # Our functionalities\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual#, FileUpload\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import FileLink, FileLinks\n",
    "# Data manipulation and calculation packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "data_frame_names = []\n",
    "cleaning_dfs = {}\n",
    "cleaning_file_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please enter your name, and a name for your experiment\n",
    "## This will help us and you to build more meaningfull file names for your exported data and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0d4b788c8c45539ea5bc253ac493a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='nwwmrdeinr', placeholder='Type your name'), Text(value='smyull', placeholder='Type …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "researcher_name = widgets.Text(\n",
    "    value=utils.get_random_string(10),\n",
    "    placeholder='Type your name'\n",
    ")\n",
    "project_name = widgets.Text(\n",
    "    value=utils.get_random_string(6),\n",
    "    placeholder='Type your project name'\n",
    ")\n",
    "\n",
    "names = widgets.VBox([researcher_name,project_name])\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings up environment variables to config your research environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[consts.PROJECT_NAME_ENV_VAR]=project_name.value\n",
    "os.environ[consts.USERNAME_ENV_VAR]=researcher_name.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) File Uploads\n",
    "## Please note we currently support only CSV files.\n",
    "##    If your'e using Excel or other file formats please convert to CSV\n",
    "###  Excel to CSV - https://knowledgebase.constantcontact.com/articles/KnowledgeBase/6409-saving-an-excel-file-as-a-csv-file?lang=en_US\n",
    "### EDF to CSV - https://emotiv.gitbook.io/emotivpro/convert_edf_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Choose Primary files to upload\n",
    "## Note -you can choose here more than one file, but the files must have the same structure or else it will fails.\n",
    "## What we will do is taking all the files and concat them together.\n",
    "\n",
    "## If you alerady uploaded the file you can select it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23f903551eb442a94308ab9d001c22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='File name:', options=('./idc_CANLab/examples/gal_data/bad_ex…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.choose_filename(filename)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_filename(filename):\n",
    "  return filename\n",
    "primary_file_name_dropdown = widgets.SelectMultiple(\n",
    "    options=utils.list_all_files_in_dir(\".\"),\n",
    "    description='File name:',\n",
    ")\n",
    "# primary_file_name_dropdown\n",
    "interact(choose_filename, filename=primary_file_name_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_file_names = list(primary_file_name_dropdown.value)\n",
    "un_concat_dfs = []\n",
    "for file_name in primary_file_names:\n",
    "  un_concat_dfs.append(pd.read_csv(file_name))\n",
    "\n",
    "dfs['primary'] = pd.concat(un_concat_dfs)\n",
    "data_frame_names = list(dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have not uploaded the file already, please upload it\n",
    "## If you used the 2 boxes above make sure to SKIP these 2 boxes, or else you will override your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_concat_dfs = []\n",
    "for file_name, byte_file in uploaded.items():\n",
    "    un_concat_dfs.append(pd.read_csv(io.StringIO(byte_file.decode(\"utf-8\"))))\n",
    "dfs['primary'] = pd.concat(un_concat_dfs)\n",
    "data_frame_names = list(dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Choose Other files to upload, files we wish to merge to the primary file, like demographic data, metadata on the trial and others\n",
    "## If you alerady uploaded the file you can select it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_filename(filename):\n",
    "  return filename\n",
    "secondary_files_name_dropdown = widgets.SelectMultiple(\n",
    "    options=utils.list_all_files_in_dir(\"/.\"),\n",
    "    description='File name:',\n",
    ")\n",
    "\n",
    "interact(choose_filename, filename=secondary_files_name_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scondary_file_names = list(secondary_files_name_dropdown.value)\n",
    "for file_name in scondary_file_names:\n",
    "  dfs[file_name] = pd.read_csv(file_name)\n",
    "data_frame_names = list(dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have not uploaded the file already, please upload it\n",
    "## If you used the 2 boxes above please skip these 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name, byte_file in merged_uploaded.items():\n",
    "  data_frame_names.append(file_name)\n",
    "  dfs[file_name] = pd.read_csv(io.StringIO(byte_file.decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Data Sumarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "https://storage.googleapis.com/outliers/nwwmrdeinr/smyull/info_07-18-2020-19%3A36%3A41.txt\n",
      "https://storage.googleapis.com/outliers/nwwmrdeinr/smyull/info_07-18-2020-19%3A36%3A44.txt\n",
      "https://storage.googleapis.com/outliers/object_nwwmrdeinr/smyull/descibe_07-18-2020-19%3A36%3A46.csv\n",
      "https://storage.googleapis.com/outliers/numeric_nwwmrdeinr/smyull/descibe_07-18-2020-19%3A36%3A46.csv\n"
     ]
    }
   ],
   "source": [
    "print(summary.info_dfs(list(dfs.values())))\n",
    "for sum_url in summary.describe_dfs(list(dfs.values())):\n",
    "    print(sum_url)\n",
    "    \n",
    "merged_df = dfs[data_frame_names[0]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Merging Extra data from external files - (for example memory span, age, etc)\n",
    "\n",
    "### Please note that if you wish to look at the raw data before adding more from external sources, all you need to do is just skip the following box and move to the plotting part\n",
    "\n",
    "### Please note, it is recommended to read a about JOIN and ANTI_JOIN opparations in SQL and Python Pandas to fully understand what we are doing here, although not mandatory. We would try and make the process as simple as possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) For each file we wish to merge, select the columns you wish to merge by with the primary file, these columns will be used as unique key and the recommendation will be the User Id/Session Label/Subject and Trial_Id/Trial Label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_colums(colums):\n",
    "  return colums\n",
    "\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "external_merge_columns_dropdown = []\n",
    "for i in range(1,len(set(data_frame_names))):\n",
    "  if data_frame_names[i] == \"primary\":\n",
    "    continue\n",
    "  w1 = widgets.SelectMultiple(\n",
    "      options=dfs[data_frame_names[i]].columns,\n",
    "      description=data_frame_names[i]\n",
    "  )\n",
    "  w2 = widgets.SelectMultiple(\n",
    "      options=dfs[data_frame_names[0]].columns,\n",
    "      description=data_frame_names[0]\n",
    "  )\n",
    "  external_merge_columns_dropdown.append(widgets.HBox([w1,w2]))\n",
    "  \n",
    "columns_map = widgets.VBox(external_merge_columns_dropdown)\n",
    "columns_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Merging is done automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = dfs[data_frame_names[0]].copy()\n",
    "for i in range(1,len(set(data_frame_names))):\n",
    "  child = columns_map.children[i-1]\n",
    "  left = list(child.children[1].value)\n",
    "  right = list(child.children[0].value)\n",
    "  merged_df = merged_df.merge(dfs[data_frame_names[i]], left_on=left, right_on=right, how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Data Sumarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(summary.info_dfs([merged_df]))\n",
    "for sum_url in summary.describe_dfs([merged_df]):\n",
    "    print(sum_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Droping unwanted colums\n",
    "## Here we suggest removing uneeded columns from our primary merged file, this could help both better performance and understanding of the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Choose unwanted colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_colums(colums):\n",
    "  return colums\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "colums_dropdown = widgets.SelectMultiple(\n",
    "    options=merged_df.columns,\n",
    "    description='Select colums to drop:',\n",
    ")\n",
    "\n",
    "interact(choose_colums, colums=colums_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Dropping the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colums_to_drop = list(colums_dropdown.value)\n",
    "merged_df = merged_df.drop(columns=colums_to_drop)\n",
    "\n",
    "# Flattening \n",
    "for col in merged_df.columns:\n",
    "  if merged_df[col].dtype == object:\n",
    "    merged_df[col] = merged_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Data Sumarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary.info_dfs([merged_df]))\n",
    "for sum_url in summary.describe_dfs([merged_df]):\n",
    "    print(sum_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) Choose columns to convert to categorical, for example subject_id, trial_id, memory span and others should be categorical\n",
    "## Wants to convert some numeric values for categorical? Do it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_colums(colums):\n",
    "  return colums\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "categorical_colums_dropdown = widgets.SelectMultiple(\n",
    "    options=merged_df.columns,\n",
    "    description='Select colums to drop:',\n",
    ")\n",
    "\n",
    "interact(choose_colums, colums=categorical_colums_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colums_to_convert_to_categorical = list(categorical_colums_dropdown.value)\n",
    "for col in colums_to_convert_to_categorical:\n",
    "  merged_df[col] = merged_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Ploting the raw data we have so far\n",
    "\n",
    "## In the next few boxes we are going through a process of plotting the trials a certain Subject was going through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Per Subject Ploting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1) Select the name of the column you wish to group by, usually the subject or trial columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_name(column_name):\n",
    "  return column_name\n",
    "\n",
    "groupped_column_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Subject:',\n",
    ")\n",
    "\n",
    "interact(col_name, column_name=groupped_column_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2) Select the desire columns you wish to have as your y and x axis.\n",
    "### For y, usually the column that follow the eye gaze on the Target, but could also be for the Competitor or one of the fillers\n",
    "### For x, usually the time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_axis(axis):\n",
    "  return axis\n",
    "\n",
    "y_column_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Select `Y` axis column name:',\n",
    ")\n",
    "x_column_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Select `X` axis column name:',\n",
    ")\n",
    "\n",
    "interact(choose_axis, axis=y_column_dropdown)\n",
    "interact(choose_axis, axis=x_column_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3) Select the different features (e.g memory span, critical, load, noise level) you want to create the grid by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_colums(colums):\n",
    "  return colums\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "features_column_dropdown = widgets.SelectMultiple(\n",
    "    options=merged_df.columns,\n",
    "    description='Select colums to drop:',\n",
    ")\n",
    "\n",
    "interact(choose_colums, colums=features_column_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.4) Plot!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupping_col_name = groupped_column_dropdown.value\n",
    "y_name = y_column_dropdown.value\n",
    "x_name = x_column_dropdown.value\n",
    "grid_features = list(features_column_dropdown.value)\n",
    "\n",
    "_, url = plots.plots_by_group_and_features(merged_df, groupping_col_name, y_name, x_name, grid_features)\n",
    "print(\"plots_by_group_and_features - \",url)\n",
    "_, url = plots.plot_general_avg_grid(merged_df, y_name, x_name, grid_features)\n",
    "print(\"plot_general_avg_grid - \",url)\n",
    "_, url = plots.plot_general_avg(merged_df, y_name, x_name)\n",
    "print(\"plot_general_avg - \",url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Filtering and Data Cleaning\n",
    "## You can repeat the folowing steps in order to clean different values from different colmuns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1) Drop rows with Nan/Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) Choose Column to filter by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_name(column_name):\n",
    "  return column_name\n",
    "\n",
    "filter_column_dropdown = widgets.Dropdown(\n",
    "    options=merged_df.columns,\n",
    "    description='Col Name:',\n",
    ")\n",
    "\n",
    "interact(col_name, column_name=filter_column_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3) Choose value to remove from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = filter_column_dropdown.value\n",
    "values = list(merged_df[col_name].unique())\n",
    "def value_name(value):\n",
    "  return value\n",
    "\n",
    "value_name_dropdown = widgets.Dropdown(\n",
    "    options=values,\n",
    "    description='Value Name:',\n",
    ")\n",
    "\n",
    "interact(value_name, value=value_name_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4) Filter that value out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = value_name_dropdown.value\n",
    "selector = (merged_df[col_name] != value)\n",
    "merged_df = merged_df[selector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5) Data Summarization and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary.info_dfs([merged_df]))\n",
    "for sum_url in summary.describe_dfs([merged_df]):\n",
    "    print(sum_url)\n",
    "\n",
    "_, url = plots.plots_by_group_and_features(merged_df, groupping_col_name, y_name, x_name, grid_features)\n",
    "print(\"plots_by_group_and_features - \",url)\n",
    "_, url = plots.plot_general_avg_grid(merged_df, y_name, x_name, grid_features)\n",
    "print(\"plot_general_avg_grid - \",url)\n",
    "_, url = plots.plot_general_avg(merged_df, y_name, x_name)\n",
    "print(\"plot_general_avg - \",url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6) Filter according to exteral files.\n",
    "## Here we will use the idea behind anti join to drop certain rows accroding to files you have prepared in advance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1) If you alerady uploaded the file you can select it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_filename(filename):\n",
    "  return filename\n",
    "anti_join_files_name_dropdown = widgets.SelectMultiple(\n",
    "    options=utils.list_all_files_in_dir(\".\"),\n",
    "    description='File name:',\n",
    ")\n",
    "\n",
    "interact(choose_filename, filename=anti_join_files_name_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_dfs = {}\n",
    "anti_join_file_names = list(anti_join_files_name_dropdown.value)\n",
    "for file_name in anti_join_file_names:\n",
    "  c_df = pd.read_csv(file_name)\n",
    "  for col in c_df.columns:\n",
    "    if col in merged_df.columns:\n",
    "      c_df[col] = c_df[col].astype(merged_df[col].dtype)\n",
    "  cleaning_dfs[file_name] = c_df\n",
    "cleaning_file_names = list(cleaning_dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2) If you have not uploaded the file already, please upload it\n",
    "### If you used the 2 boxes above please skip these 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_merge_uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name, byte_file in anti_merge_uploaded.items():\n",
    "  c_df = pd.read_csv(io.StringIO(byte_file.decode(\"utf-8\")))\n",
    "  for col in c_df.columns:\n",
    "    if col in merged_df.colmuns:\n",
    "      c_df[col] = c_df[col].astype(merged_df[col].dtype)\n",
    "        \n",
    "  cleaning_dfs[file_name] = c_df\n",
    "cleaning_file_names = list(cleaning_dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3) Choose the unique column names you wish to anti-join.\n",
    "### This columns hsould create a unique like key so we can tell our notebook to remove each row that corresponds from the file to the data we have cleaning and merged so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_colums(colums):\n",
    "  return colums\n",
    "\n",
    "# Widget that will be shwon at the bottom, after running the box\n",
    "external_clean_columns_dropdown = []\n",
    "for i in range(0,len(cleaning_file_names)):\n",
    "  w1 = widgets.SelectMultiple(\n",
    "      options=cleaning_dfs[cleaning_file_names[i]].columns,\n",
    "      description=cleaning_file_names[i]\n",
    "  )\n",
    "  w2 = widgets.SelectMultiple(\n",
    "      options=merged_df.columns,\n",
    "      description='primary data frame'\n",
    "  )\n",
    "  external_clean_columns_dropdown.append(widgets.HBox([w1,w2]))\n",
    "  \n",
    "columns_map = widgets.VBox(external_clean_columns_dropdown)\n",
    "columns_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cleaning_file_names)):\n",
    "  clean_df = cleaning_dfs[cleaning_file_names[i]]\n",
    "  child = columns_map.children[i-1]\n",
    "  left = list(child.children[1].value)\n",
    "  right = list(child.children[0].value)\n",
    "  merged_df = utils.anti_join(merged_df, clean_df, left_on=left, right_on=right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.4) Data Summarization and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary.info_dfs([merged_df]))\n",
    "for sum_url in summary.describe_dfs([merged_df]):\n",
    "    print(sum_url)\n",
    "\n",
    "_, url = plots.plots_by_group_and_features(merged_df, groupping_col_name, y_name, x_name, grid_features)\n",
    "print(\"plots_by_group_and_features - \",url)\n",
    "_, url = plots.plot_general_avg_grid(merged_df, y_name, x_name, grid_features)\n",
    "print(\"plot_general_avg_grid - \",url)\n",
    "_, url = plots.plot_general_avg(merged_df, y_name, x_name)\n",
    "print(\"plot_general_avg - \",url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Saving files\n",
    "## At any stage you can choose to save the current state of our manipulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1) Full backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck_merged_df = merged_df.copy()\n",
    "urls = saved_file_name = backup.save_locally_and_update(merged_df, dfs, cleaning_dfs)\n",
    "for url in urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2) Diff backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = saved_file_name = backup.save_locally_and_update(merged_df)\n",
    "for url in urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
